{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import (BaseEstimator, TransformerMixin)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used for this analysis will be a 2009 survey conducted for the H1N1 outbreak. This survey was performed by the CDC in order to monitor and evaluate the flu vaccination efforts of adults and children in randomly selected US households. The questions asked of the participants dealt with their H1N1 vaccination status, flu-related behaviors, opinions about flu vaccine safety and effectivenss, recent respiratory illness, and pneumococcal vaccination status <a href=\"#About the National Immunization Survery\">[1]</a>.\n",
    "\n",
    "The following data from the survey can be found and downloaded <a href=\"https://www.drivendata.org/competitions/66/flu-shot-learning/data/\">here</a><a href=\"#Source Data Download\">[2]</a> with feature descriptions found <a href=\"https://github.com/cschneck7/Iterative_Classification_Blog/blob/main/data/H1N1_and_Seasonal_Flu_Vaccines_Feature_Information.txt\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import survey data into dataframes\n",
    "# The source dataset already had this split feature and target files\n",
    "X = pd.read_csv('data/source_data/training_set_features.csv')\n",
    "y = pd.read_csv('data/source_data/training_set_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are originally two different target variable, for this example we will only concentrate on `h1n1_vaccine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets target variable\n",
    "y = y.h1n1_vaccine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick look at feature dataframe shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26707, 36)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns shape of feature dataframe\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick look at missing values in feature dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "respondent_id                      0\n",
       "h1n1_concern                      92\n",
       "h1n1_knowledge                   116\n",
       "behavioral_antiviral_meds         71\n",
       "behavioral_avoidance             208\n",
       "behavioral_face_mask              19\n",
       "behavioral_wash_hands             42\n",
       "behavioral_large_gatherings       87\n",
       "behavioral_outside_home           82\n",
       "behavioral_touch_face            128\n",
       "doctor_recc_h1n1                2160\n",
       "doctor_recc_seasonal            2160\n",
       "chronic_med_condition            971\n",
       "child_under_6_months             820\n",
       "health_worker                    804\n",
       "health_insurance               12274\n",
       "opinion_h1n1_vacc_effective      391\n",
       "opinion_h1n1_risk                388\n",
       "opinion_h1n1_sick_from_vacc      395\n",
       "opinion_seas_vacc_effective      462\n",
       "opinion_seas_risk                514\n",
       "opinion_seas_sick_from_vacc      537\n",
       "age_group                          0\n",
       "education                       1407\n",
       "race                               0\n",
       "sex                                0\n",
       "income_poverty                  4423\n",
       "marital_status                  1408\n",
       "rent_or_own                     2042\n",
       "employment_status               1463\n",
       "hhs_geo_region                     0\n",
       "census_msa                         0\n",
       "household_adults                 249\n",
       "household_children               249\n",
       "employment_industry            13330\n",
       "employment_occupation          13470\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks amount of Nan values in feature dataframe\n",
    "missing_values = X.isna().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Frequent Entry Imputation\n",
    "\n",
    "Below we observe the normalized distribution of a feature missing only a few entries and one containing many missed entries. We will use these two features to observe how using most frequent entry imputation is good for features that are almost complete though creates a bias for features missing most entries. This bias is very noticable if the original distribution is almost evenly spread, while less severe and possibly usable at distributions that are very far apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values: 19\n",
      "0.0    0.931018\n",
      "1.0    0.068982\n",
      "Name: behavioral_face_mask, dtype: float64\n",
      "Number of missing values: 12274\n",
      "1.0    0.87972\n",
      "0.0    0.12028\n",
      "Name: health_insurance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Takes both a feature with little and many missing entries\n",
    "X_missing = X[['behavioral_face_mask', 'health_insurance']]\n",
    "\n",
    "# Displays number of missing values as well as normalized\n",
    "# value distribution of existing values in percentages\n",
    "print(f'Number of missing values: {X_missing.behavioral_face_mask.isna().sum()}')\n",
    "print(X_missing.behavioral_face_mask.value_counts(normalize=True))\n",
    "print(f'Number of missing values: {X_missing.health_insurance.isna().sum()}')\n",
    "print(X_missing.health_insurance.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing the most frequent entry using SimpleImputer then analyzing the new distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values: 0\n",
      "0.0    0.931067\n",
      "1.0    0.068933\n",
      "Name: behavioral_face_mask, dtype: float64\n",
      "Number of missing values: 0\n",
      "1.0    0.934998\n",
      "0.0    0.065002\n",
      "Name: health_insurance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Creating a simple imputer object with strategy of most_frequent\n",
    "si = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "# creates dataframe of transformed features\n",
    "X_most_frequent = pd.DataFrame(data=si.fit_transform(X_missing),\n",
    "                               index=X_missing.index,\n",
    "                               columns=X_missing.columns)\n",
    "\n",
    "\n",
    "# Displays new value distributions after imputation\n",
    "print(f'Number of missing values: {X_most_frequent.behavioral_face_mask.isna().sum()}')\n",
    "print(X_most_frequent.behavioral_face_mask.value_counts(normalize=True))\n",
    "print(f'Number of missing values: {X_most_frequent.health_insurance.isna().sum()}')\n",
    "print(X_most_frequent.health_insurance.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above this imputation method barely changed the distribution of values for `behavioural_face_mask` which was only missing 19 entries. On the other hand the feature `health_insurance` which was missing nearly half its values had its distibution spread increase by nearly 11%. Even though there was already a mismatched distribution the most frequent entry imputation method created a larger bias in value distribution.\n",
    "\n",
    "## Random Imputation\n",
    "\n",
    "Next we will take a quick look at random imputation. This method randomly imputes values based of the existing values distribution. This may already be more attractive than the previous method because it ensures the distribution will stay constant. We'll use the same values as before to provide an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behavioral_face_mask\n",
      "\n",
      "Original Distribution:\n",
      "Number of missing values: 19\n",
      "0.0    0.931018\n",
      "1.0    0.068982\n",
      "Name: behavioral_face_mask, dtype: float64\n",
      "\n",
      "Distribution after random imputation:\n",
      "Number of missing values: 0\n",
      "0.0    0.931029\n",
      "1.0    0.068971\n",
      "Name: behavioral_face_mask, dtype: float64\n",
      "\n",
      "-------------------------------------------------------------\n",
      "\n",
      "health_insurance\n",
      "\n",
      "Original Distribution:\n",
      "Number of missing values: 12274\n",
      "1.0    0.87972\n",
      "0.0    0.12028\n",
      "Name: health_insurance, dtype: float64\n",
      "\n",
      "Distribution after random imputation:\n",
      "Number of missing values: 0\n",
      "1.0    0.879957\n",
      "0.0    0.120043\n",
      "Name: health_insurance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Creates copy of DataFrame\n",
    "X_rand_imp = X_missing.copy()\n",
    "\n",
    "# Iterates through features\n",
    "for col in X_missing.columns:\n",
    "#     Finds number of missing values in feature\n",
    "    number_missing = X_rand_imp[col].isnull().sum()\n",
    "#     Finds normalized distribution of existing entries\n",
    "    value_dist = X_rand_imp.loc[X_rand_imp[col].notnull(), col].value_counts(normalize=True)\n",
    "#     Sets random seed for random.choice\n",
    "    np.random.seed(0)\n",
    "#     Randomly Imputes observed values replacing all missing information\n",
    "    X_rand_imp.loc[X_rand_imp[col].isnull(), col] = np.random.choice(value_dist.index, \n",
    "                                                                     number_missing, \n",
    "                                                                     replace = True,\n",
    "                                                                     p = value_dist)\n",
    "    \n",
    "# Displays before and after imputation distributions\n",
    "print('behavioral_face_mask\\n')\n",
    "print(f'Original Distribution:\\nNumber of missing values: {X_missing.behavioral_face_mask.isna().sum()}')\n",
    "print(X_missing.behavioral_face_mask.value_counts(normalize=True))\n",
    "print(f'\\nDistribution after random imputation:\\nNumber of missing values: {X_rand_imp.behavioral_face_mask.isna().sum()}')\n",
    "print(X_rand_imp.behavioral_face_mask.value_counts(normalize=True))\n",
    "print('\\n-------------------------------------------------------------\\n')\n",
    "print('health_insurance\\n')\n",
    "print(f'Original Distribution:\\nNumber of missing values: {X_missing.health_insurance.isna().sum()}')\n",
    "print(X_missing.health_insurance.value_counts(normalize=True))\n",
    "print(f'\\nDistribution after random imputation:\\nNumber of missing values: {X_rand_imp.health_insurance.isna().sum()}')\n",
    "print(X_rand_imp.health_insurance.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above this method maintains the original distribution of values after imputation. It should be noted that while this method maintains the distribution of the original feature, the accuracy of imputed entries drops in comparison to the most frequent entry imputation approach. If you assume the missing entries follow the same distribution as the original, by imputing the most frequent entry the accuracy will be equal to the distribution of that most frequent entry in the original data. For example if the distribution is 80:20 for existing values, the accuracy of most frequent entry imputation will be 80%. While the accuracy of using the above random imputation method is only 68%. This can be found by considering of the 80% of values randomly imputed as the most frequent value, only 80% of them will be correct and vice versa for the less frequent value. The accuracy for random imputation is calculated below for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.00000000000001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*((.8*.8) + (.2*.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if you had a feature similar to the H1N1 survey. Some features have many missing values so most frequent entry imputation would not work there. Also random imputation may not be ideal because of the lower accuracy as well as the random aspect. Why not try a method that will use the other features to help predict what these missing values could be. Atleast that was my mentality while working on this project, therefore I went searching for a model to fit my case. I had one issue that had to be handled prior to trying to classify my features. This was the different amount of missing values present in the features and in different entry locations. Any classification model won't work if it's features have NaN values, thus these need to be sorted prior to modeling. I wasn't sure exactly how to handle this but found an <a href=\"https://www.kaggle.com/code/shashankasubrahmanya/missing-data-imputation-using-regression\">article</a> written by Shashanka Subrahmanya were he uses regression to fill in missing values <a href='#Missing Data Imputation using Regression'>[3]</a>. In Shashanka's article he utilizes random imputation prior to creating his model to then attempt to predict those missing values therefore I took the same approach. I took it a step further though and created my own class with a fit and transform methods in order to save the value distributions if a training and test split is being used.\n",
    "\n",
    "## Random Imputation Class Creation\n",
    "\n",
    "I start my RandomImputer class by inheriting the properties of `BaseEstimator` and `TransformerMixin`. The initialization of the class can take one argument `missing_columns`. This argument can be used to specidy which columns are to have random imputation performed on them. If left to the default value any column with a missing value will be performed on.\n",
    "\n",
    "//Insert initializaiton steps\n",
    "\n",
    "Next the `fit` method is created, since this class is a transformer only the feature DataFrame is required as an argument and `y` is set equal to `None`. In the `fit` method the columns to be transformed are will be assigned if they weren't specified in the initialization. The value distributions are then saved in a dictionary and saved as a parameter to be utilized in the transform method.\n",
    "\n",
    "//Insert fit method\n",
    "\n",
    "Next is the transform method. In the transform method, values are randomly imputed in the locations with missing values. New columns are created to signal that these columns have imputed data. The original columns with missing values are not removed because they are later used in the Iterative Classification model to locate where the missing values were located. Returned is a DataFrame containing the passed columns to the transformer as well as new columns with the randomly imputed values.\n",
    "\n",
    "// Insert transform method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomImputer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Randomly imputes values for missing data in new columns.\n",
    "    Values are based off of existing values and rates of occurences.\n",
    "    \n",
    "    Initialized with optional argument columns which specify\n",
    "    columns to be transformed, if left to default value 'all_missing_columns', all\n",
    "    columns with missing values will be filled.\n",
    "    '''\n",
    "    \n",
    "#     Initializes class object\n",
    "    def __init__(self, missing_columns='all_missing_columns'):\n",
    "        self.missing_columns = missing_columns\n",
    " \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "#     Finds column names containing missing values if missing_columns equals all_missing_columns\n",
    "        if self.missing_columns == 'all_missing_columns':\n",
    "            nan_amount = X.isna().sum()\n",
    "            self.missing_columns = list(nan_amount[nan_amount>0].index)\n",
    "#     Handles if single column entered as string\n",
    "#     str type changed to list for future for loop operation\n",
    "        elif type(self.missing_columns) == str:\n",
    "            self.missing_columns = [self.missing_columns]\n",
    "\n",
    "#     Initializes empty dict which will contain distribution of existing values for columns\n",
    "#     in missing_columns\n",
    "        feature_value_info = {}\n",
    "#     Iterates through missing_columns finding value distributions \n",
    "        for col in self.missing_columns:\n",
    "            feature_value_info[col] = X.loc[X[col].notnull(), col].value_counts(normalize=True)\n",
    "\n",
    "#     Saves distributions as parameter\n",
    "        self.feature_value_info = feature_value_info\n",
    "        return self\n",
    "            \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "#         Sets random seed for random seed generation for iterative\n",
    "#         calls to random.choice in for loop\n",
    "        np.random.seed(7337)\n",
    "#         random seeds generation for loop\n",
    "        rand_seeds = np.random.randint(0, 10e3, len(self.missing_columns), 'int64')\n",
    "        \n",
    "        df = X.copy()\n",
    "        \n",
    "#     Iterates through missing columns\n",
    "        for i, col in enumerate(self.missing_columns):\n",
    "#     Creates copy of column to have values imputed into\n",
    "            df[col+'_imp'] = df[col]\n",
    "#     Finds number of missing values in column\n",
    "            number_missing = df[col].isnull().sum()\n",
    "#     Sets random seed for random.choice\n",
    "            np.random.seed(rand_seeds[i])\n",
    "#     Randomly Imputes observed values replacing all missing information\n",
    "            df.loc[df[col].isnull(), col+'_imp'] = np.random.choice(self.feature_value_info[col].index, \n",
    "                                                                    number_missing, \n",
    "                                                                    replace = True,\n",
    "                                                                    p = self.feature_value_info[col])\n",
    "\n",
    "#     Creates column index variable to be called to set DataFrame index\n",
    "        self.features_out = df.columns\n",
    "        \n",
    "        return df\n",
    "    \n",
    "#     Returns final columns index\n",
    "    def get_features_out(self):\n",
    "        return self.features_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this transformer to the dataset and observe the results. First we need to perform some feature preperation that includes encoding our object columns and dropping `respondent_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops id column\n",
    "X_mod = X.drop('respondent_id', axis=1)\n",
    "\n",
    "# Creates dataframe of object types\n",
    "X_obj_feat = X_mod.select_dtypes(include='object').copy()\n",
    "\n",
    "# Creates dataframe non object features\n",
    "X_num_feat = X_mod.select_dtypes(exclude='object').copy()\n",
    "\n",
    "# Initialized ordinal encoder object\n",
    "oe = OrdinalEncoder()\n",
    "\n",
    "# Creates dataframe of ordinal encoded data\n",
    "X_enc_obj = pd.DataFrame(data = oe.fit_transform(X_obj_feat),\n",
    "                         columns=X_obj_feat.columns,\n",
    "                         index=X_obj_feat.index)\n",
    "\n",
    "# Combines encoded object features with non-object features\n",
    "X_enc = pd.concat([X_enc_obj, X_num_feat], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that are object features are encoded we can now move on to the random imputation transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "age_group                          0\n",
       "race                               0\n",
       "sex                                0\n",
       "hhs_geo_region                     0\n",
       "census_msa                         0\n",
       "education_imp                      0\n",
       "income_poverty_imp                 0\n",
       "marital_status_imp                 0\n",
       "rent_or_own_imp                    0\n",
       "employment_status_imp              0\n",
       "employment_industry_imp            0\n",
       "employment_occupation_imp          0\n",
       "h1n1_concern_imp                   0\n",
       "h1n1_knowledge_imp                 0\n",
       "behavioral_antiviral_meds_imp      0\n",
       "behavioral_avoidance_imp           0\n",
       "behavioral_face_mask_imp           0\n",
       "behavioral_wash_hands_imp          0\n",
       "behavioral_large_gatherings_imp    0\n",
       "behavioral_outside_home_imp        0\n",
       "behavioral_touch_face_imp          0\n",
       "doctor_recc_h1n1_imp               0\n",
       "doctor_recc_seasonal_imp           0\n",
       "chronic_med_condition_imp          0\n",
       "child_under_6_months_imp           0\n",
       "health_worker_imp                  0\n",
       "health_insurance_imp               0\n",
       "opinion_h1n1_vacc_effective_imp    0\n",
       "opinion_h1n1_risk_imp              0\n",
       "opinion_h1n1_sick_from_vacc_imp    0\n",
       "opinion_seas_vacc_effective_imp    0\n",
       "opinion_seas_risk_imp              0\n",
       "opinion_seas_sick_from_vacc_imp    0\n",
       "household_adults_imp               0\n",
       "household_children_imp             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates class object\n",
    "ri = RandomImputer()\n",
    "\n",
    "# Fits and transforms dataset\n",
    "X_imp = ri.fit_transform(X_enc)\n",
    "\n",
    "print('Missing values:')\n",
    "# checks if all columns besides original columns with missing data are filled in\n",
    "X_imp.drop(ri.missing_columns, axis=1).isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now filled all missing values using random imputation. In the next blog we will use an iterative classification class to impute the missing values based off of the feature content.\n",
    "\n",
    "// Potentially break to next blog with just the iterative classification class. If passable add a proper conclusion and introduction to this blog.\n",
    "\n",
    "## Iterative Classification Method\n",
    "\n",
    "Next we'll take a look at the iterative classifier. Similarly to the random imputation a class was created to prevent data leakage in case a train and test split is being utilized. The classifier works by iterating through the features and creating a seperate Decision Tree models at each step. During each step, one feature is determined as the dependent variable and the rest are the independent variables. These are then used to fit a Decision Tree Classifier. This model is then used to predict values to replace those that were randomly imputed using the RandomImputer class. The next iteration then utilizes these newly predicted values to build the model for the next feature. This is then repeated until all features with missing values have had models created for them as dependent variables. Let's take a look at the creation of this class.\n",
    "\n",
    "### Initialization\n",
    "\n",
    "The IterativeClassification class is initialized with four possible arguments:\n",
    "\n",
    "1. `max_depth` to define the depth of the DecisionTreeClassifiers.\n",
    "2. `class_order` determining the order that features are classified in. Currently the two available options are `many_first` and `less_first`. The `many_first` option will classify features in descending order of missing values in the feature, while `less_first` is in ascending order.\n",
    "3. `num_cols` contains list of numerical features\n",
    "4. `cat_cols` contains list of categorical features.\n",
    "\n",
    "The arguments `num_cols` and `cat_cols` are required in order to perform a OneHotEncoding transformer on any categorical variables before each iterations modeling process.\n",
    "\n",
    "// Insert initialization part of code\n",
    "\n",
    "### Helper Methods\n",
    "\n",
    "The class has two helper methods `_imp_columns` and `_step_features`.\n",
    "\n",
    "#### `_imp_columns`\n",
    "\n",
    "This helper method takes a list of feature names and adds the `_imp` extension to them if they had previously had values imputed for their missing values using the RandomImputer transformer. This is mainly used to set the list of independent variable names for each iteration.\n",
    "\n",
    "// insert _imp_columns method\n",
    "\n",
    "#### `_step_features`\n",
    "\n",
    "This method is used to return the current iterations independent variables. It takes two arguments, `X` which is the feature dataframe and `col` which is the current iterations target variable. It then utilizes the `_imp_columns` function to create a list of the proper feature names for both categorical and numericals features assigned by the `cat_cols` and `num_cols` parameters. The categorical columns are also One Hot Encoded during this step. This method also makes sure to sort the features in order to prevent errors occuring in the calculations using the Decision Tree Classifier. \n",
    "\n",
    "// insert _step_features method\n",
    "\n",
    "### `fit` Method\n",
    " \n",
    "The `fit` method is called with a single argument `X` which is the feature DataFrame. This DataFrame can even contain the original problems target variable to help predict the reamining missing values. The method starts out by creating a list of columns that had missing values before the RandomImputater transformer was used. This list is sorted in either descending or ascending order by quantity of missing values depending on the parameter `class_order` and is saved as the parameter `missing_columns`. \n",
    "\n",
    "// Insert up to the end of missing_columns sorting\n",
    "\n",
    "Next the remaining features that had no missing values are found, sorted and saved to the parameter `leftover_features`. \n",
    "\n",
    "// Insert finding leftover_features\n",
    "\n",
    "Utilizing list comprehension the extension `_imp` is added to the features present in `missing_columns`. This list is combined with the list of features in `leftover_features` and saved the parameter `pred_features` which will be used to make a slice of `X` containing all the features to be used in creating the iterative decision tree models. This DataFrame called `pred_df` now contains the columns with the randomly imputed data and the features that don't contain any missing data. \n",
    "\n",
    "// Insert through creation of pred_df\n",
    "\n",
    "Next two empty dictionaries are initiated to contain each iterations Decisiont Tree Classification model as well as that iterations accuracy. The accuracy is calculated using the values that weren't missing compared to their predicted values. The features contained in `missing_colunns` are then iterated through, setting the current feature as the target variable. The `_step_features` helper method is used to create each iterations independent variable dataframe. A decision tree classification model is then fit to this iterations data and saved to the `models` dictionary. Before moving to the next iteration the `pred_df` is updated to replace the current iterations target variables randomly imputed values with predicted values using the decision tree classifier. An accuracy score is then calculated and added to the `accuracy_scores` dictionary. Once all the features contained in `missing_columns` have been iterated through the two dictionaries are saved as parameters and `self` is returned to complete the `fit` method.\n",
    "\n",
    "// Insert end of fit method\n",
    "\n",
    "### `transform` Method\n",
    "\n",
    "The `transform` method starts by creating a new holder dataframe that will contain the original and predicted values. The dataframe is initialized to contain the feature names contained in the parameter `missing_columns` with `Det_` added to the front of each name. This is to signify that this feature has been modified using the IterativeClassification transformer. Then similarly to the `fit` method each feature included in the parameter `missing_columns` is iterated through. In each iteration the helper method `_step_features` is used create the current encoded feature dataframe. This dataframe is then used to predict the and replace the randomly imputed values using the decision tree models from the `transform` method. After all iterations are finished the final dataframe with all randomly imputed values replaced with prediced values is returned.\n",
    "\n",
    "// insert transform method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterativeClassification(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Uses an iterative DecisionTreeClassifier to fill in missing values.\n",
    "    \n",
    "    __init__ :\n",
    "        Input: max_depth = max_depth parameter of DecisionTreeClassifier\n",
    "               class_order = 'many_first' or 'less_first'\n",
    "                             default = 'many_first', determines which order\n",
    "                             or columns to iterate through based off of quantity\n",
    "                             of missing values in columns\n",
    "               num_cols = columns with numerical data\n",
    "               cat_cols = columns with categorical data to be one hot encoded\n",
    "    \n",
    "    fit : \n",
    "        Input: X = DataFrame with missing values\n",
    "               y = None\n",
    "               \n",
    "    transform :\n",
    "        Input: X = DataFrame with missing values to be transformed\n",
    "        \n",
    "    get_features_out :\n",
    "        Returns column names for final transformed DataFrame\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, max_depth=None, class_order='many_first', num_cols=None, cat_cols=None):\n",
    "        self.max_depth=max_depth\n",
    "        self.class_order=class_order\n",
    "        self.num_cols=num_cols\n",
    "        self.cat_cols=cat_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        '''\n",
    "        Iteratively fits DecisionTreeClassifier models to pd.DataFrame X\n",
    "        \n",
    "        1. Sorts columns with missing values by quantity, the order set\n",
    "        by argument class_order with options 'many_first' and 'less_first'.\n",
    "        \n",
    "        2. Finds the leftover features that didn't have any missing values\n",
    "        and don't need to be classified from DataFrame X, sorts these to prevent\n",
    "        indexing issues later on.\n",
    "        \n",
    "        3 Creates a list of all features being used to fit the \n",
    "        DecisionTreeClassifier, input DataFrame X should be in a format resulting\n",
    "        from being tranformed using the RandomImputer tranformer. The features \n",
    "        resulting from this transformer have had variables randomly imputed and \n",
    "        the feature names ending with '_imp' to signify their transformation. \n",
    "        The input DataFrame X contains all the features from the original DataFrame\n",
    "        with missing values plus the imputed features.\n",
    "        \n",
    "        4. Creates a DataFrame copy using the list from step 3.\n",
    "        \n",
    "        5. Iterates through the columns that had missing values,\n",
    "        using their imputed copy as a target variable and the rest of \n",
    "        the features from the DataFrame created in step 4 to fit a \n",
    "        DecisionTreeClassifier. The remaining features have to be \n",
    "        OneHotEncoded before used in Classifier. This classifier is then \n",
    "        used to predict the variables for those that were originally \n",
    "        missing and replacing them in the DataFrame created in step 4.\n",
    "        \n",
    "        The fifth step is then repeated for each column while saving the \n",
    "        DecisionTreeClassifier at each step to be used later in the \n",
    "        transform method.\n",
    "        '''\n",
    "        \n",
    "#         Finds amount of nans in columns\n",
    "#         Orders columns to be predicted by class_order argument\n",
    "        nan_amount = X.isna().sum()\n",
    "        if self.class_order == 'many_first':\n",
    "            self.missing_columns = list(nan_amount[nan_amount>0].sort_values(ascending=False).index)\n",
    "        elif self.class_order == 'less_first':\n",
    "            self.missing_columns = list(nan_amount[nan_amount>0].sort_values().index)\n",
    "        else:\n",
    "            sys.exit('''Incorrect input for class_order argument.\n",
    "                 Argument only accepts values ('many_first', or 'less_first')''')\n",
    "\n",
    "#         Finds features in dataframe that aren't included in missing_columns\n",
    "        leftover_features = list(set(X.columns) - set(self.missing_columns) - \n",
    "                                {col+'_imp' for col in self.missing_columns})\n",
    "        leftover_features.sort()\n",
    "        self.leftover_features = leftover_features\n",
    "        \n",
    "#         Sets missing_columns predictive features with _imp extension matching fitting from RandomImputer class\n",
    "#         Adds leftover features to create dataframe with all predictive features\n",
    "        self.pred_features = [col+'_imp' for col in self.missing_columns] + leftover_features\n",
    "        pred_df = X[self.pred_features].copy()\n",
    "        \n",
    "#         Creates empty dictionairies for fitted models, and their accuracy scores\n",
    "        models = {}\n",
    "        accuracy_scores = {}\n",
    "        \n",
    "#         Iterates through missing_columns, creating a model, updating feature with predicted values\n",
    "#         at each iteration\n",
    "        for col in self.missing_columns:\n",
    "#             creates dataframe for this iterations predictives features\n",
    "#             This involves ohe categorical features\n",
    "            temp_df = self._step_features(pred_df, col)\n",
    "#             Initializes and fits DecisionTreeClassifier\n",
    "            dt = DecisionTreeClassifier(max_depth=self.max_depth, random_state=42)\n",
    "            dt.fit(temp_df, pred_df[col+'_imp'])\n",
    "            \n",
    "#             imports predicted values into missing locations\n",
    "            pred_df.loc[X[col].isnull(), col+'_imp'] = dt.predict(temp_df)[X[col].isnull()]\n",
    "            \n",
    "#             Saves this iterations model and accuracy score\n",
    "            models[col] = dt\n",
    "            accuracy_scores[col] = accuracy_score(X.loc[X[col].notnull(), col],\n",
    "                                                dt.predict(temp_df)[X[col].notnull()])    \n",
    "        self.dt_models = models\n",
    "        self.fit_accuracy_scores = accuracy_scores\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        '''\n",
    "        Transforms X using models fit in fit method\n",
    "        \n",
    "        1. Creates prediction df with all columns that will be classified\n",
    "        \n",
    "        2. Iterates through the columns with missing values in the same manner\n",
    "           as the fit method, except only tranforming the target using the\n",
    "           fitted classifiers from the fit method.\n",
    "           \n",
    "        3. Creates Final DataFrame containing classified columns, plus columns\n",
    "           that weren't originally missing values\n",
    "        '''\n",
    "#         Initializes dataframe containing determined values\n",
    "        det_df = pd.DataFrame(columns = ['Det_'+col for col in self.missing_columns])\n",
    "#         Creates dataframe of predictive features\n",
    "        pred_df = X[self.pred_features].copy()\n",
    "        \n",
    "#         Iterates through missing columns to transform with fitted models\n",
    "        for col in self.missing_columns:\n",
    "#             Fills determined column with values\n",
    "            det_df['Det_'+col] = X[col+'_imp']\n",
    "#             creates dataframe for this iterations predictives features\n",
    "#             This involves ohe categorical features \n",
    "            temp_df = self._step_features(pred_df, col)\n",
    "#             Loads this iterations fitted model\n",
    "            dt_model = self.dt_models[col]\n",
    "#            imports predicted values into missing locations for predictive and determined dataframe\n",
    "            det_df.loc[X[col].isnull(), 'Det_'+col] = dt_model.predict(temp_df)[X[col].isnull()]\n",
    "            pred_df.loc[X[col].isnull(), col+'_imp'] = dt_model.predict(temp_df)[X[col].isnull()]\n",
    "\n",
    "#         Creates transfromed df determined features as well as nontransformed features\n",
    "        final_df = pd.concat([det_df, X[self.leftover_features]], axis=1)\n",
    "        \n",
    "#         Saves transformed models feature names        \n",
    "        self.features_out = final_df.columns\n",
    "    \n",
    "        return final_df\n",
    "    \n",
    "#     Returns transformed models feature names\n",
    "    def get_features_out(self):\n",
    "        return self.features_out\n",
    "    \n",
    "#     Creates dataframe for the current iteration\n",
    "    def _step_features(self, X, col):\n",
    "        '''\n",
    "        Takes current iterations target variable, and creates\n",
    "        independent variable dataframe with proper feature names \n",
    "        and OneHotEncodes categorical columns\n",
    "        \n",
    "        inputs: X = feature dataframe\n",
    "                col = Current iterations target variable\n",
    "                \n",
    "        output: DataFrame with OneHotEncoded categorical features\n",
    "                and correct feature names matching those from the\n",
    "                RandomImputer class\n",
    "        '''\n",
    "#         Checks if categorical features exist\n",
    "        if self.cat_cols != None:\n",
    "#             adds _imp to end of feature name if included in missing_columns\n",
    "            imp_cat_cols = self._imp_columns(self.cat_cols)\n",
    "#             creates list of columns to be OneHotEncoded\n",
    "            ohe_cols = list(set(imp_cat_cols) - {col+'_imp'})\n",
    "#             Sorts in alphabetical order for consistency\n",
    "            ohe_cols.sort()\n",
    "#             Initializes, fits and transforms OneHotEncoder\n",
    "            ohe = OneHotEncoder(sparse=False) \n",
    "            ohe_features = ohe.fit_transform(X[ohe_cols])\n",
    "#             Creates dataframe with OneHotEncoded features\n",
    "            ohe_df = pd.DataFrame(ohe_features,\n",
    "                        columns=ohe.get_feature_names_out(ohe_cols),\n",
    "                        index=X.index)\n",
    "#             Checks if there are numerical columns    \n",
    "            if self.num_cols != None:\n",
    "#                 adds _imp to end of feature name if included in missing_columns\n",
    "                imp_num_cols = self._imp_columns(self.num_cols)\n",
    "#                 Creates list of numerical column names and sorts them\n",
    "                num_columns = list(set(imp_num_cols) - {col+'_imp'})\n",
    "                num_columns.sort()\n",
    "#                 creates dataframe with ohe and num features    \n",
    "                temp_df = pd.concat([X[num_columns], ohe_df], axis=1)\n",
    "#             Creates dataframe if there are no numerical features\n",
    "            else:\n",
    "                temp_df = ohe_df\n",
    "#         Checks if their are numerical features, if no categorical features            \n",
    "        elif self.num_cols != None:\n",
    "#             adds _imp to end of feature name if included in missing_columns\n",
    "            imp_num_cols = self._imp_columns(self.num_cols)\n",
    "#             Creates list of numerical column names and sorts them\n",
    "            num_columns = list(set(imp_num_cols) - {col+'_imp'})\n",
    "            num_columns.sort()\n",
    "            \n",
    "#             creates dataframe with numerical features with altered names\n",
    "            temp_df = X[num_columns].copy()\n",
    "#         If no features were assigned to either num_cols or cat_cols\n",
    "        else: \n",
    "            sys.exit('''Need to assign both or one of num_cols and cat_cols''')\n",
    "            \n",
    "        return temp_df\n",
    "    \n",
    "    def _imp_columns(self, columns):\n",
    "        '''\n",
    "        Adds _imp to end of feature name if included in missing_columns.\n",
    "        missing_columns should be features transformed through the RandomImputer\n",
    "        class\n",
    "        \n",
    "        input: columns = columns to have feature names tranformed if applicable\n",
    "        ouput: list of new column names\n",
    "        '''\n",
    "        imp_cols = [col+'_imp' if col in self.missing_columns else col for col in columns]\n",
    "        return imp_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our IterativeClassification transformer is complete, let's check how it works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical feature names\n",
    "cat_cols = ['race',\n",
    "            'hhs_geo_region',\n",
    "            'census_msa',\n",
    "            'employment_status',\n",
    "            'household_adults',\n",
    "            'household_children',\n",
    "            'education',\n",
    "            'income_poverty']\n",
    "\n",
    "# numerical feature names\n",
    "num_cols = list(set(X.columns) - set(cat_cols) - {'respondent_id'})\n",
    "\n",
    "# Creates IterativeClassification class object with max_depth = 5\n",
    "iter_class = IterativeClassification(max_depth=5, cat_cols=cat_cols, num_cols=num_cols)\n",
    "\n",
    "# fit and transformes data\n",
    "X_prep = iter_class.fit_transform(X_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataframes with missing values have now been successfuly filled using an iterative classification model. This class can easily be modified to utilize seperate models, or be used in conjuction with other imputation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] <a id='About the National Immunization Survery' href=\"https://webarchive.loc.gov/all/20140511031000/http://www.cdc.gov/nchs/nis/about_nis.htm#h1n1\">https://webarchive.loc.gov/all/20140511031000/http://www.cdc.gov/nchs/nis/about_nis.htm#h1n1</a>\n",
    "\n",
    "[2] <a href='https://www.drivendata.org/competitions/66/flu-shot-learning/data/'>https://www.drivendata.org/competitions/66/flu-shot-learning/data/</a>\n",
    "\n",
    "[3] <a id='Missing Data Imputation using Regression' href='https://www.kaggle.com/code/shashankasubrahmanya/missing-data-imputation-using-regression'>https://www.kaggle.com/code/shashankasubrahmanya/missing-data-imputation-using-regression</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
